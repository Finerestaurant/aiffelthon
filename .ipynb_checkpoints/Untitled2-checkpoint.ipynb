{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186cd88f-4dfa-4f29-9602-1686a6ebb97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading dataset...\n",
      "Loaded data : 93389\n",
      "\n",
      "batch_size = 16\n",
      "learning rate = 0.0001\n",
      "train_size = 74711\n",
      "test_size = 18678\n",
      "Data load complete!\n",
      "\n",
      "Initializing model....\n",
      "Initialize complete!!\n",
      "\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "ename": "ScopeParamShapeError",
     "evalue": "Inconsistent shapes between value and initializer for parameter \"kernel\" in \"/encoder/Conv_0\": (3, 3, 1876, 512), (3, 3, 1, 512). (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 89>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m test_x, test_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(test_data)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# x = x + 100\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# test_x = test_x + 100\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m state, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m           \n\u001b[1;32m    154\u001b[0m _, test_loss, mse_loss, kld_loss \u001b[38;5;241m=\u001b[39m eval_step(state, test_x, rng)\n\u001b[1;32m    156\u001b[0m recon_x, _, _, _ \u001b[38;5;241m=\u001b[39m eval_step(state, target, rng)\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(state, x, z_rng)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m     73\u001b[0m grad_fn \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvalue_and_grad(loss_fn)\n\u001b[0;32m---> 74\u001b[0m loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mapply_gradients(grads\u001b[38;5;241m=\u001b[39mgrads), loss\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mtrain_step.<locals>.loss_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(params):\n\u001b[0;32m---> 66\u001b[0m     recon_x, mean, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mConv2d_VAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_rng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     mse_loss \u001b[38;5;241m=\u001b[39m ((recon_x \u001b[38;5;241m-\u001b[39m x)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     69\u001b[0m     kld_loss \u001b[38;5;241m=\u001b[39m kl_divergence(mean, logvar)\u001b[38;5;241m.\u001b[39mmean()\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m~/module/Conv2d_model.py:118\u001b[0m, in \u001b[0;36mConv2d_VAE.__call__\u001b[0;34m(self, x, z_rng)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, z_rng):\n\u001b[1;32m    115\u001b[0m  \n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# x = x.reshape(*x.shape, 1)\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     z, mean, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_rng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     recon_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m recon_x, mean, logvar\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/module/Conv2d_model.py:20\u001b[0m, in \u001b[0;36mEncoder.__call__\u001b[0;34m(self, x, z_rng)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, z_rng):\n\u001b[0;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mleaky_relu(x)\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mnormalization\u001b[38;5;241m.\u001b[39mBatchNorm(\u001b[38;5;28;01mTrue\u001b[39;00m)(x)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py:415\u001b[0m, in \u001b[0;36m_Conv.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m kernel_shape:\n\u001b[1;32m    412\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMask needs to have the same shape as weights. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    413\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShapes are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 415\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkernel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m   kernel \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/core/scope.py:768\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, *init_args)\u001b[0m\n\u001b[1;32m    763\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m val, abs_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(value_flat, abs_value_flat):\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;66;03m# NOTE: We could check dtype consistency here as well but it's\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;66;03m# usefuleness is less obvious. We might intentionally change the dtype\u001b[39;00m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m# for inference to a half float type for example.\u001b[39;00m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mshape(val) \u001b[38;5;241m!=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mshape(abs_val):\n\u001b[0;32m--> 768\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamShapeError(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text,\n\u001b[1;32m    769\u001b[0m                                         jnp\u001b[38;5;241m.\u001b[39mshape(val), jnp\u001b[38;5;241m.\u001b[39mshape(abs_val))\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mutable_collection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mScopeParamShapeError\u001b[0m: Inconsistent shapes between value and initializer for parameter \"kernel\" in \"/encoder/Conv_0\": (3, 3, 1876, 512), (3, 3, 1, 512). (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
     ]
    }
   ],
   "source": [
    "from dataloader import mel_dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from Conv2d_model import Conv2d_VAE\n",
    "from linear_evaluation import linear_evaluation\n",
    "\n",
    "import flax \n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    x_train = [x for x, _ in batch]\n",
    "    y_train = [y for _, y in batch]                  \n",
    "        \n",
    "    return np.array(x_train), np.array(y_train)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 16\n",
    "    lr = 0.0001\n",
    "    rng = jax.random.PRNGKey(303)\n",
    "    \n",
    "    print('\\n')\n",
    "    # ---Load dataset---\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset_dir = os.path.join(os.path.expanduser('~'),'dataset')\n",
    "    data = mel_dataset(dataset_dir)\n",
    "    print(f'Loaded data : {len(data)}\\n')\n",
    "    target = data[0][0] # (48, 1876)\n",
    "    target = jnp.expand_dims(target, axis = 0)\n",
    "    \n",
    "    dataset_size = len(data)\n",
    "    train_size = int(dataset_size * 0.8)\n",
    "    test_size = dataset_size - train_size\n",
    "    \n",
    "    train_dataset, test_dataset = random_split(data, [train_size, test_size])\n",
    "\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_batch)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=int(batch_size/4), shuffle=True, num_workers=0, collate_fn=collate_batch)\n",
    "    \n",
    "    print(f'batch_size = {batch_size}')\n",
    "    print(f'learning rate = {lr}')\n",
    "    print(f'train_size = {train_size}')\n",
    "    print(f'test_size = {test_size}')\n",
    "    \n",
    "   \n",
    "    \n",
    "    print(\"Initialize complete!!\\n\")\n",
    "    # ---train model---\n",
    "    epoch = 1\n",
    "    # checkpoint_dir = str(input('checkpoint dir : '))\n",
    "    \n",
    "    \n",
    "    for i in range(epoch):\n",
    "        train_data = iter(train_dataloader)\n",
    "        test_data = iter(test_dataloader)\n",
    "        \n",
    "        train_loss_mean = 0\n",
    "        test_loss_mean = 0\n",
    "        \n",
    "        \n",
    "        print(f'\\nEpoch {i+1}')\n",
    "        \n",
    "        for j in range(len(train_dataloader)):\n",
    "            rng, key = jax.random.split(rng)\n",
    "            x, y = next(train_data)\n",
    "            test_x, test_y = next(test_data)\n",
    "            \n",
    "            # x = x + 100\n",
    "            # test_x = test_x + 100\n",
    "            \n",
    "            state, train_loss = train_step(state, x, rng)           \n",
    "            _, test_loss, mse_loss, kld_loss = eval_step(state, test_x, rng)\n",
    "            \n",
    "            recon_x, _, _, _ = eval_step(state, target, rng)\n",
    "            train_loss_mean += train_loss\n",
    "            test_loss_mean += test_loss\n",
    "            \n",
    "                        \n",
    "        \n",
    "                \n",
    "                \n",
    "            print(f'step : {j}/{len(train_dataloader)}, train_loss : {round(train_loss, 3)}, test_loss : {round(test_loss, 3)}', end='\\r')\n",
    "\n",
    "        print(f'epoch {i+1} - average loss - train : {round(train_loss_mean/len(train_dataloader), 3)}, test : {round(test_loss_mean/len(test_dataloader), 3)}')\n",
    "        \n",
    "    # ---Linear evaluation---\n",
    "    \n",
    "           \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f5195a-4fce-46b3-b72c-03dd0300fdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 1475346432 bytes == 0x10fc30000 @  0x7f1251b11680 0x7f1251b32824 0x7f1251b32b8a 0x7f10b0ad6a8c 0x7f10ac252b35 0x7f10ac256acd 0x7f10ac1902a9 0x7f10abf597fd 0x7f10abf42781 0x5f6929 0x5f74f6 0x50c383 0x570b26 0x569dba 0x5f6eb3 0x5f8892 0x66931d 0x5f627e 0x56d2d5 0x569dba 0x5f6eb3 0x5f6082 0x56d2d5 0x569dba 0x5f6eb3 0x5f8892 0x66931d 0x5f627e 0x56d2d5 0x5f6cd6 0x56bbfa\n",
      "tcmalloc: large alloc 1475354624 bytes == 0x167b30000 @  0x7f1251b11680 0x7f1251b32824 0x7f10ac9cb045 0x7f10aca6cf46 0x7f10aca6d718 0x7f1014774096 0x7f10ac256acd 0x7f10ac1902a9 0x7f10abf597fd 0x7f10abf42781 0x5f6929 0x5f74f6 0x50c383 0x570b26 0x569dba 0x5f6eb3 0x5f8892 0x66931d 0x5f627e 0x56d2d5 0x569dba 0x5f6eb3 0x5f6082 0x56d2d5 0x569dba 0x5f6eb3 0x5f8892 0x66931d 0x5f627e 0x56d2d5 0x5f6cd6\n",
      "tcmalloc: large alloc 1475346432 bytes == 0x167b30000 @  0x7f1251b11680 0x7f1251b32824 0x7f1251b32b8a 0x7f10b0ad6a8c 0x7f10ac252b35 0x7f10ac256acd 0x7f10ac1902a9 0x7f10abf597fd 0x7f10abf42781 0x5f6929 0x5f74f6 0x50c383 0x570b26 0x569dba 0x5f6eb3 0x5f882b 0x66931d 0x5f627e 0x56d2d5 0x569dba 0x5f6eb3 0x5f6082 0x56d2d5 0x5f6cd6 0x56bbfa 0x569dba 0x5f6eb3 0x5f6082 0x56d2d5 0x569dba 0x5f6eb3\n",
      "tcmalloc: large alloc 3688800256 bytes == 0x1bfa32000 @  0x7f1251b11680 0x7f1251b32824 0x7f10ac9cb045 0x7f10ac9e1441 0x7f10ac9e23b0 0x7f10ac9e2dab 0x7f10aca6badb 0x7f10aca6cf59 0x7f10aca6d718 0x7f101476e0a7 0x7f10ac256acd 0x7f10ac1902a9 0x7f10abf597fd 0x7f10abf42781 0x5f6929 0x5f74f6 0x50c383 0x570b26 0x569dba 0x5f6eb3 0x5f8892 0x66931d 0x5f627e 0x56d2d5 0x569dba 0x5f6eb3 0x5f6082 0x56d2d5 0x569dba 0x5f6eb3 0x5f8892\n"
     ]
    }
   ],
   "source": [
    "# ---initializing model---\n",
    "model = Conv2d_VAE()\n",
    "print(\"Initializing model....\")\n",
    "state = init_state(model, \n",
    "                   np.expand_dims(next(iter(train_dataloader))[0], axis=-1).shape,\n",
    "                   rng, \n",
    "                   lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bbec2c-2644-47b8-bcfe-834cecff7d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e366fbe-5a8d-4ae4-8614-6bd26ba5527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_params = state.params['params']['encoder']\n",
    "enc_batch_stats = state.params['batch_stats']['encoder']\n",
    "\n",
    "@jax.jit\n",
    "def latent_vector(data):\n",
    "    whole_latent = []\n",
    "    for linear_data in data:\n",
    "        linear_data = jnp.expand_dims(linear_data, axis=0)\n",
    "        latent_vector = Encoder().apply({'params':enc_params,'batch_stats':enc_batch_stats}, linear_data, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
