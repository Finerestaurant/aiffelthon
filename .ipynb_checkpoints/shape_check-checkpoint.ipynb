{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e7a1442-5cb7-4c68-bcb0-90c28405dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022-09-02 16:23 Seoul\n",
    "\n",
    "# --- import dataset ---\n",
    "from utils.dataloader import mel_dataset\n",
    "from utils.losses import *\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# --- import model ---\n",
    "from model.supervised_model import *\n",
    "from model.Conv1d_model import Conv1d_VAE\n",
    "# from model.Conv1d_model import Encoder as Encoder1d\n",
    "\n",
    "from model.Conv2d_model import Conv2d_VAE\n",
    "from model.Conv2d_model import Encoder\n",
    "\n",
    "# --- import framework ---\n",
    "import flax \n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "import cloudpickle\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a6875c-5b52-4fdc-bb06-9e8ba830d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- collate batch for dataloader ---\n",
    "def collate_batch(batch):\n",
    "    x_train = [x for x, _ in batch]\n",
    "    y_train = [y for _, y in batch]                  \n",
    "        \n",
    "    return np.array(x_train), np.array(y_train)\n",
    "\n",
    "\n",
    "\n",
    "# --- define init state ---\n",
    "def init_state(model, x_shape, key, lr) -> train_state.TrainState:\n",
    "    params = model.init({'params': key}, jnp.ones(x_shape), key)\n",
    "    optimizer = optax.adam(learning_rate=lr)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        tx=optimizer,\n",
    "        params=params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- define train_step ---\n",
    "@jax.jit\n",
    "def train_step(state, x, z_rng):    \n",
    "    \n",
    "    def loss_fn(params):\n",
    "        recon_x, mean, logvar = model.apply(params, x, z_rng)\n",
    "        kld_loss = kl_divergence(mean, logvar).mean()\n",
    "        mse_loss = ((recon_x - x)**2).mean()\n",
    "        loss = mse_loss + kld_loss\n",
    "        return loss\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    \n",
    "    return state.apply_gradients(grads=grads), loss\n",
    "\n",
    "@jax.jit\n",
    "def linear_train_step(encoder, \n",
    "                      enc_state, \n",
    "                      enc_batch, \n",
    "                      linear_state, \n",
    "                      x, y):    \n",
    "    \n",
    "    latent = encoder.apply({'params':enc_state, 'batch_stats':enc_batch}, x)\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        logits = linear_evaluation().apply(params, latent)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits, y))\n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(linear_state.params)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == jnp.argmax(labels, -1))\n",
    "    \n",
    "    return state.apply_gradients(grads=grads), loss\n",
    "\n",
    "\n",
    "\n",
    "# --- define eval step ---\n",
    "@jax.jit\n",
    "def eval_step(state, x, z_rng):\n",
    "    \n",
    "    recon_x, mean, logvar = model.apply(state.params, x, z_rng)\n",
    "    kld_loss = kl_divergence(mean, logvar).mean()\n",
    "    mse_loss = ((recon_x - x)**2).mean()\n",
    "    loss = mse_loss + kld_loss\n",
    "    \n",
    "    return recon_x, loss, mse_loss, kld_loss\n",
    "\n",
    "@jax.jit\n",
    "def linear_eval_step(state, x, y):\n",
    "    \n",
    "    logits = linear_evaluation().apply(state.params, x)\n",
    "    loss = jnp.mean(optax.softmax_cross_entropy(logits, y))\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == jnp.argmax(labels, -1))\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8d8eb-cdd1-42bc-b84a-ba8ad2078936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Load song_meta.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 707989/707989 [00:00<00:00, 822937.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load complete!\n",
      "\n",
      "Load file list...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:07, 14.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data : 93389\n",
      "\n",
      "Data load complete!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                     Conv2d_VAE Summary                                      </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> path                  </span>┃<span style=\"font-weight: bold\"> outputs              </span>┃<span style=\"font-weight: bold\"> batch_stats        </span>┃<span style=\"font-weight: bold\"> params                </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Inputs                │ -                    │                    │                       │\n",
       "│                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[16,48,1876]  │                    │                       │\n",
       "│                       │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">uint32</span>[2]          │                    │                       │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/BatchNorm_0   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,12,469,32] │ mean: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[32]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[32]     │\n",
       "│                       │                      │ var: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[32]   │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[32]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ <span style=\"font-weight: bold\">64 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(256 B)</span>         │ <span style=\"font-weight: bold\">64 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(256 B)</span>            │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/BatchNorm_1   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,12,469,64] │ mean: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]     │\n",
       "│                       │                      │ var: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]   │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ <span style=\"font-weight: bold\">128 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(512 B)</span>        │ <span style=\"font-weight: bold\">128 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(512 B)</span>           │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/BatchNorm_2   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,24,938,12… │ mean: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]    │\n",
       "│                       │                      │ var: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]  │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]   │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ <span style=\"font-weight: bold\">256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 KB)</span>       │ <span style=\"font-weight: bold\">256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 KB)</span>          │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/ConvTranspos… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,12,469,32] │                    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[32]     │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,1,32]     │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ <span style=\"font-weight: bold\">320 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.3 KB)</span>          │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/ConvTranspos… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,12,469,64] │                    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]     │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,32,64]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ <span style=\"font-weight: bold\">18,496 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(74.0 KB)</span>      │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/ConvTranspos… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,24,938,12… │                    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]    │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,64,128]   │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ <span style=\"font-weight: bold\">73,856 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(295.4 KB)</span>     │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/ConvTranspos… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,48,1876,2… │                    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]    │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,128,256]  │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ <span style=\"font-weight: bold\">295,168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.2 MB)</span>      │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/ConvTranspos… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,48,1876,1] │                    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1]      │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,256,1]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ <span style=\"font-weight: bold\">2,305 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(9.2 KB)</span>        │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/Dense_0       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,5628]      │                    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[5628]   │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,5628]     │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ <span style=\"font-weight: bold\">2,887,164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(11.5 MB)</span>   │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder               │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,48,1876,1] │                    │                       │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/BatchNorm_0   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,24,512]    │ mean: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]    │\n",
       "│                       │                      │ var: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]  │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]   │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>     │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>        │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/BatchNorm_1   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,24,512]    │ mean: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]    │\n",
       "│                       │                      │ var: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]  │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]   │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>     │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>        │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/BatchNorm_2   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,12,256]    │ mean: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]    │\n",
       "│                       │                      │ var: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]  │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]   │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ <span style=\"font-weight: bold\">512 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(2.0 KB)</span>       │ <span style=\"font-weight: bold\">512 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(2.0 KB)</span>          │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/BatchNorm_3   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,12,128]    │ mean: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]    │\n",
       "│                       │                      │ var: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]  │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]   │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ <span style=\"font-weight: bold\">256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 KB)</span>       │ <span style=\"font-weight: bold\">256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 KB)</span>          │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/BatchNorm_4   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,12,64]     │ mean: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]     │\n",
       "│                       │                      │ var: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]   │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ <span style=\"font-weight: bold\">128 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(512 B)</span>        │ <span style=\"font-weight: bold\">128 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(512 B)</span>           │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/BatchNorm_5   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,12,32]     │ mean: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[32]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[32]     │\n",
       "│                       │                      │ var: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[32]   │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[32]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ <span style=\"font-weight: bold\">64 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(256 B)</span>         │ <span style=\"font-weight: bold\">64 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(256 B)</span>            │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/BatchNorm_6   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,12,16]     │ mean: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[16]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[16]     │\n",
       "│                       │                      │ var: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[16]   │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[16]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ <span style=\"font-weight: bold\">32 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(128 B)</span>         │ <span style=\"font-weight: bold\">32 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(128 B)</span>            │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/BatchNorm_7   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,12,1]      │ mean: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1]      │\n",
       "│                       │                      │ var: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1]    │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1]     │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ <span style=\"font-weight: bold\">2 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(8 B)</span>            │ <span style=\"font-weight: bold\">2 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(8 B)</span>               │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/Conv_0        │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,24,512]    │                    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]    │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,1876,512] │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ <span style=\"font-weight: bold\">8,645,120 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(34.6 MB)</span>   │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/Conv_1        │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,24,512]    │                    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]    │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,512,512]  │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ <span style=\"font-weight: bold\">2,359,808 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(9.4 MB)</span>    │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/Conv_2        │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,12,256]    │                    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]    │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,512,256]  │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ <span style=\"font-weight: bold\">1,179,904 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.7 MB)</span>    │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/Conv_3        │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,12,128]    │                    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]    │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,256,128]  │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ <span style=\"font-weight: bold\">295,040 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.2 MB)</span>      │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/Conv_4        │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,12,64]     │                    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]     │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,128,64]   │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ <span style=\"font-weight: bold\">73,792 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(295.2 KB)</span>     │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/Conv_5        │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,12,32]     │                    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[32]     │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,64,32]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ <span style=\"font-weight: bold\">18,464 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(73.9 KB)</span>      │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/Conv_6        │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,12,16]     │                    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[16]     │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,32,16]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ <span style=\"font-weight: bold\">4,624 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(18.5 KB)</span>       │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/Conv_7        │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,12,1]      │                    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1]      │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,16,1]     │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ <span style=\"font-weight: bold\">145 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(580 B)</span>           │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/fc3_logvar    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,512]       │                    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]    │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[12,512]       │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ <span style=\"font-weight: bold\">6,656 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(26.6 KB)</span>       │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/fc3_mean      │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,512]       │                    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]    │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[12,512]       │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ <span style=\"font-weight: bold\">6,656 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(26.6 KB)</span>       │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder               │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,512]     │                    │                       │\n",
       "│                       │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,512]     │                    │                       │\n",
       "│                       │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,512]     │                    │                       │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ Conv2d_VAE            │ -                    │                    │                       │\n",
       "│                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,48,1876,1] │                    │                       │\n",
       "│                       │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,512]     │                    │                       │\n",
       "│                       │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,512]     │                    │                       │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│<span style=\"font-weight: bold\">                       </span>│<span style=\"font-weight: bold\">                Total </span>│<span style=\"font-weight: bold\"> 3,490 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(14.0 KB)</span><span style=\"font-weight: bold\">    </span>│<span style=\"font-weight: bold\"> 15,871,008 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(63.5 MB)</span><span style=\"font-weight: bold\">  </span>│\n",
       "└───────────────────────┴──────────────────────┴────────────────────┴───────────────────────┘\n",
       "<span style=\"font-weight: bold\">                                                                                             </span>\n",
       "<span style=\"font-weight: bold\">                           Total Parameters: 15,874,498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(63.5 MB)</span><span style=\"font-weight: bold\">                            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                     Conv2d_VAE Summary                                      \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mpath                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mbatch_stats       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams               \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Inputs                │ -                    │                    │                       │\n",
       "│                       │ \u001b[2mfloat32\u001b[0m[16,48,1876]  │                    │                       │\n",
       "│                       │ - \u001b[2muint32\u001b[0m[2]          │                    │                       │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/BatchNorm_0   │ \u001b[2mfloat32\u001b[0m[4,12,469,32] │ mean: \u001b[2mfloat32\u001b[0m[32]  │ bias: \u001b[2mfloat32\u001b[0m[32]     │\n",
       "│                       │                      │ var: \u001b[2mfloat32\u001b[0m[32]   │ scale: \u001b[2mfloat32\u001b[0m[32]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m         │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m            │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/BatchNorm_1   │ \u001b[2mfloat32\u001b[0m[4,12,469,64] │ mean: \u001b[2mfloat32\u001b[0m[64]  │ bias: \u001b[2mfloat32\u001b[0m[64]     │\n",
       "│                       │                      │ var: \u001b[2mfloat32\u001b[0m[64]   │ scale: \u001b[2mfloat32\u001b[0m[64]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ \u001b[1m128 \u001b[0m\u001b[1;2m(512 B)\u001b[0m        │ \u001b[1m128 \u001b[0m\u001b[1;2m(512 B)\u001b[0m           │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/BatchNorm_2   │ \u001b[2mfloat32\u001b[0m[4,24,938,12… │ mean: \u001b[2mfloat32\u001b[0m[128] │ bias: \u001b[2mfloat32\u001b[0m[128]    │\n",
       "│                       │                      │ var: \u001b[2mfloat32\u001b[0m[128]  │ scale: \u001b[2mfloat32\u001b[0m[128]   │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ \u001b[1m256 \u001b[0m\u001b[1;2m(1.0 KB)\u001b[0m       │ \u001b[1m256 \u001b[0m\u001b[1;2m(1.0 KB)\u001b[0m          │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/ConvTranspos… │ \u001b[2mfloat32\u001b[0m[4,12,469,32] │                    │ bias: \u001b[2mfloat32\u001b[0m[32]     │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ \u001b[2mfloat32\u001b[0m[3,3,1,32]     │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ \u001b[1m320 \u001b[0m\u001b[1;2m(1.3 KB)\u001b[0m          │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/ConvTranspos… │ \u001b[2mfloat32\u001b[0m[4,12,469,64] │                    │ bias: \u001b[2mfloat32\u001b[0m[64]     │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ \u001b[2mfloat32\u001b[0m[3,3,32,64]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ \u001b[1m18,496 \u001b[0m\u001b[1;2m(74.0 KB)\u001b[0m      │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/ConvTranspos… │ \u001b[2mfloat32\u001b[0m[4,24,938,12… │                    │ bias: \u001b[2mfloat32\u001b[0m[128]    │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ \u001b[2mfloat32\u001b[0m[3,3,64,128]   │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ \u001b[1m73,856 \u001b[0m\u001b[1;2m(295.4 KB)\u001b[0m     │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/ConvTranspos… │ \u001b[2mfloat32\u001b[0m[4,48,1876,2… │                    │ bias: \u001b[2mfloat32\u001b[0m[256]    │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ \u001b[2mfloat32\u001b[0m[3,3,128,256]  │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ \u001b[1m295,168 \u001b[0m\u001b[1;2m(1.2 MB)\u001b[0m      │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/ConvTranspos… │ \u001b[2mfloat32\u001b[0m[4,48,1876,1] │                    │ bias: \u001b[2mfloat32\u001b[0m[1]      │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ \u001b[2mfloat32\u001b[0m[3,3,256,1]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ \u001b[1m2,305 \u001b[0m\u001b[1;2m(9.2 KB)\u001b[0m        │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder/Dense_0       │ \u001b[2mfloat32\u001b[0m[4,5628]      │                    │ bias: \u001b[2mfloat32\u001b[0m[5628]   │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ \u001b[2mfloat32\u001b[0m[512,5628]     │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ \u001b[1m2,887,164 \u001b[0m\u001b[1;2m(11.5 MB)\u001b[0m   │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ decoder               │ \u001b[2mfloat32\u001b[0m[4,48,1876,1] │                    │                       │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/BatchNorm_0   │ \u001b[2mfloat32\u001b[0m[8,24,512]    │ mean: \u001b[2mfloat32\u001b[0m[512] │ bias: \u001b[2mfloat32\u001b[0m[512]    │\n",
       "│                       │                      │ var: \u001b[2mfloat32\u001b[0m[512]  │ scale: \u001b[2mfloat32\u001b[0m[512]   │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m     │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m        │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/BatchNorm_1   │ \u001b[2mfloat32\u001b[0m[8,24,512]    │ mean: \u001b[2mfloat32\u001b[0m[512] │ bias: \u001b[2mfloat32\u001b[0m[512]    │\n",
       "│                       │                      │ var: \u001b[2mfloat32\u001b[0m[512]  │ scale: \u001b[2mfloat32\u001b[0m[512]   │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m     │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m        │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/BatchNorm_2   │ \u001b[2mfloat32\u001b[0m[4,12,256]    │ mean: \u001b[2mfloat32\u001b[0m[256] │ bias: \u001b[2mfloat32\u001b[0m[256]    │\n",
       "│                       │                      │ var: \u001b[2mfloat32\u001b[0m[256]  │ scale: \u001b[2mfloat32\u001b[0m[256]   │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ \u001b[1m512 \u001b[0m\u001b[1;2m(2.0 KB)\u001b[0m       │ \u001b[1m512 \u001b[0m\u001b[1;2m(2.0 KB)\u001b[0m          │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/BatchNorm_3   │ \u001b[2mfloat32\u001b[0m[4,12,128]    │ mean: \u001b[2mfloat32\u001b[0m[128] │ bias: \u001b[2mfloat32\u001b[0m[128]    │\n",
       "│                       │                      │ var: \u001b[2mfloat32\u001b[0m[128]  │ scale: \u001b[2mfloat32\u001b[0m[128]   │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ \u001b[1m256 \u001b[0m\u001b[1;2m(1.0 KB)\u001b[0m       │ \u001b[1m256 \u001b[0m\u001b[1;2m(1.0 KB)\u001b[0m          │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/BatchNorm_4   │ \u001b[2mfloat32\u001b[0m[4,12,64]     │ mean: \u001b[2mfloat32\u001b[0m[64]  │ bias: \u001b[2mfloat32\u001b[0m[64]     │\n",
       "│                       │                      │ var: \u001b[2mfloat32\u001b[0m[64]   │ scale: \u001b[2mfloat32\u001b[0m[64]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ \u001b[1m128 \u001b[0m\u001b[1;2m(512 B)\u001b[0m        │ \u001b[1m128 \u001b[0m\u001b[1;2m(512 B)\u001b[0m           │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/BatchNorm_5   │ \u001b[2mfloat32\u001b[0m[4,12,32]     │ mean: \u001b[2mfloat32\u001b[0m[32]  │ bias: \u001b[2mfloat32\u001b[0m[32]     │\n",
       "│                       │                      │ var: \u001b[2mfloat32\u001b[0m[32]   │ scale: \u001b[2mfloat32\u001b[0m[32]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m         │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m            │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/BatchNorm_6   │ \u001b[2mfloat32\u001b[0m[4,12,16]     │ mean: \u001b[2mfloat32\u001b[0m[16]  │ bias: \u001b[2mfloat32\u001b[0m[16]     │\n",
       "│                       │                      │ var: \u001b[2mfloat32\u001b[0m[16]   │ scale: \u001b[2mfloat32\u001b[0m[16]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ \u001b[1m32 \u001b[0m\u001b[1;2m(128 B)\u001b[0m         │ \u001b[1m32 \u001b[0m\u001b[1;2m(128 B)\u001b[0m            │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/BatchNorm_7   │ \u001b[2mfloat32\u001b[0m[4,12,1]      │ mean: \u001b[2mfloat32\u001b[0m[1]   │ bias: \u001b[2mfloat32\u001b[0m[1]      │\n",
       "│                       │                      │ var: \u001b[2mfloat32\u001b[0m[1]    │ scale: \u001b[2mfloat32\u001b[0m[1]     │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │ \u001b[1m2 \u001b[0m\u001b[1;2m(8 B)\u001b[0m            │ \u001b[1m2 \u001b[0m\u001b[1;2m(8 B)\u001b[0m               │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/Conv_0        │ \u001b[2mfloat32\u001b[0m[8,24,512]    │                    │ bias: \u001b[2mfloat32\u001b[0m[512]    │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ \u001b[2mfloat32\u001b[0m[3,3,1876,512] │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ \u001b[1m8,645,120 \u001b[0m\u001b[1;2m(34.6 MB)\u001b[0m   │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/Conv_1        │ \u001b[2mfloat32\u001b[0m[8,24,512]    │                    │ bias: \u001b[2mfloat32\u001b[0m[512]    │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ \u001b[2mfloat32\u001b[0m[3,3,512,512]  │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ \u001b[1m2,359,808 \u001b[0m\u001b[1;2m(9.4 MB)\u001b[0m    │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/Conv_2        │ \u001b[2mfloat32\u001b[0m[4,12,256]    │                    │ bias: \u001b[2mfloat32\u001b[0m[256]    │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ \u001b[2mfloat32\u001b[0m[3,3,512,256]  │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ \u001b[1m1,179,904 \u001b[0m\u001b[1;2m(4.7 MB)\u001b[0m    │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/Conv_3        │ \u001b[2mfloat32\u001b[0m[4,12,128]    │                    │ bias: \u001b[2mfloat32\u001b[0m[128]    │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ \u001b[2mfloat32\u001b[0m[3,3,256,128]  │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ \u001b[1m295,040 \u001b[0m\u001b[1;2m(1.2 MB)\u001b[0m      │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/Conv_4        │ \u001b[2mfloat32\u001b[0m[4,12,64]     │                    │ bias: \u001b[2mfloat32\u001b[0m[64]     │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ \u001b[2mfloat32\u001b[0m[3,3,128,64]   │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ \u001b[1m73,792 \u001b[0m\u001b[1;2m(295.2 KB)\u001b[0m     │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/Conv_5        │ \u001b[2mfloat32\u001b[0m[4,12,32]     │                    │ bias: \u001b[2mfloat32\u001b[0m[32]     │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ \u001b[2mfloat32\u001b[0m[3,3,64,32]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ \u001b[1m18,464 \u001b[0m\u001b[1;2m(73.9 KB)\u001b[0m      │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/Conv_6        │ \u001b[2mfloat32\u001b[0m[4,12,16]     │                    │ bias: \u001b[2mfloat32\u001b[0m[16]     │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ \u001b[2mfloat32\u001b[0m[3,3,32,16]    │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ \u001b[1m4,624 \u001b[0m\u001b[1;2m(18.5 KB)\u001b[0m       │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/Conv_7        │ \u001b[2mfloat32\u001b[0m[4,12,1]      │                    │ bias: \u001b[2mfloat32\u001b[0m[1]      │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ \u001b[2mfloat32\u001b[0m[3,3,16,1]     │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ \u001b[1m145 \u001b[0m\u001b[1;2m(580 B)\u001b[0m           │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/fc3_logvar    │ \u001b[2mfloat32\u001b[0m[4,512]       │                    │ bias: \u001b[2mfloat32\u001b[0m[512]    │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ \u001b[2mfloat32\u001b[0m[12,512]       │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ \u001b[1m6,656 \u001b[0m\u001b[1;2m(26.6 KB)\u001b[0m       │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder/fc3_mean      │ \u001b[2mfloat32\u001b[0m[4,512]       │                    │ bias: \u001b[2mfloat32\u001b[0m[512]    │\n",
       "│                       │                      │                    │ kernel:               │\n",
       "│                       │                      │                    │ \u001b[2mfloat32\u001b[0m[12,512]       │\n",
       "│                       │                      │                    │                       │\n",
       "│                       │                      │                    │ \u001b[1m6,656 \u001b[0m\u001b[1;2m(26.6 KB)\u001b[0m       │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ encoder               │ - \u001b[2mfloat32\u001b[0m[4,512]     │                    │                       │\n",
       "│                       │ - \u001b[2mfloat32\u001b[0m[4,512]     │                    │                       │\n",
       "│                       │ - \u001b[2mfloat32\u001b[0m[4,512]     │                    │                       │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│ Conv2d_VAE            │ -                    │                    │                       │\n",
       "│                       │ \u001b[2mfloat32\u001b[0m[4,48,1876,1] │                    │                       │\n",
       "│                       │ - \u001b[2mfloat32\u001b[0m[4,512]     │                    │                       │\n",
       "│                       │ - \u001b[2mfloat32\u001b[0m[4,512]     │                    │                       │\n",
       "├───────────────────────┼──────────────────────┼────────────────────┼───────────────────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1m                     \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m               Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m3,490 \u001b[0m\u001b[1;2m(14.0 KB)\u001b[0m\u001b[1m   \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m15,871,008 \u001b[0m\u001b[1;2m(63.5 MB)\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m│\n",
       "└───────────────────────┴──────────────────────┴────────────────────┴───────────────────────┘\n",
       "\u001b[1m                                                                                             \u001b[0m\n",
       "\u001b[1m                           Total Parameters: 15,874,498 \u001b[0m\u001b[1;2m(63.5 MB)\u001b[0m\u001b[1m                            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Initializing model....\n",
      "Initialize complete!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "lr = 0.0001\n",
    "dilation = True\n",
    "\n",
    "\n",
    "model = Conv2d_VAE(dilation=dilation)\n",
    "rng = jax.random.PRNGKey(303)\n",
    "\n",
    "\n",
    "# ---Load dataset---\n",
    "dataset_dir = os.path.join(os.path.expanduser('~'),'dataset')            \n",
    "\n",
    "print(\"Loading dataset...\")    \n",
    "data = mel_dataset(dataset_dir)\n",
    "print(f'Loaded data : {len(data)}\\n')\n",
    "\n",
    "dataset_size = len(data)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(data, [train_size, test_size])\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=int(batch_size/4), shuffle=True, num_workers=0, collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "print('Data load complete!\\n')\n",
    "print(nn.tabulate(model, rngs={'params': rng})(next(iter(train_dataloader))[0], rng))\n",
    "\n",
    "\n",
    "\n",
    "# ---initializing model---\n",
    "print(\"Initializing model....\")\n",
    "state = init_state(model, \n",
    "                   next(iter(train_dataloader))[0].shape, \n",
    "                   rng, \n",
    "                   lr)\n",
    "\n",
    "print(\"Initialize complete!!\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e75a43a2-7dea-4908-b70a-e1596a52c321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_init_state(model, x_shape, key, lr) -> train_state.TrainState:\n",
    "    params = model.init({'params': key}, jnp.ones(x_shape))\n",
    "    optimizer = optax.adam(learning_rate=lr)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        tx=optimizer,\n",
    "        params=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51316686-29af-42e4-905e-904d52c5c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = iter(train_dataloader)\n",
    "test_data = iter(test_dataloader)\n",
    "\n",
    "train_loss_mean = 0\n",
    "test_loss_mean = 0\n",
    "\n",
    "print(f'\\nEpoch {i+1}')\n",
    "\n",
    "for j in range(len(train_dataloader)):\n",
    "    rng, key = jax.random.split(rng)\n",
    "    x, y = next(train_data)\n",
    "    test_x, test_y = next(test_data)\n",
    "\n",
    "    x = (x + 100) \n",
    "    test_x = (test_x + 100)\n",
    "\n",
    "    state, train_loss = train_step(state, x, rng)           \n",
    "    recon_x, test_loss, mse_loss, kld_loss = eval_step(state, test_x, rng)\n",
    "    train_loss_mean += train_loss\n",
    "    test_loss_mean += test_loss\n",
    "\n",
    "    print(f'step : {j}/{len(train_dataloader)}, train_loss : {round(train_loss, 3)}, test_loss : {round(test_loss, 3)}', end='\\r')\n",
    "\n",
    "print(f'epoch {i+1} - average loss - train : {round(train_loss_mean/len(train_dataloader), 3)}, test : {round(test_loss_mean/len(test_dataloader), 3)}')\n",
    "\n",
    "\n",
    "print('Pre train complete!\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e547aa9d-68b0-4a4b-bc53-4da855633f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def linear_train_step(enc_state, \n",
    "                      enc_batch, \n",
    "                      linear_state, \n",
    "                      x, y):    \n",
    "    \n",
    "    latent, _, _ = Encoder2d().apply({'params':enc_state, 'batch_stats':enc_batch}, x, rng)\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        logits = linear_evaluation().apply(params, latent)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits, y))\n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(linear_state.params)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == jnp.argmax(labels, -1))\n",
    "    \n",
    "    return state.apply_gradients(grads=grads), loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8abd9078-d67a-4691-af0e-c5fb061d20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_state = linear_init_state(linear_evaluation(), (batch_size, 30), rng, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e620ffc-7847-4868-be86-92921d6bb905",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47cb6c29-7a8a-4d63-abe3-6bcebd77ad00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear evalutaion step.\n"
     ]
    }
   ],
   "source": [
    "print('Linear evalutaion step.')\n",
    "\n",
    "enc_state = state.params['params']['encoder']\n",
    "enc_batch = state.params['batch_stats']['encoder']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2b20e21-5c52-4882-ba4e-79ed84992c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = iter(train_dataloader)\n",
    "test_data = iter(test_dataloader)\n",
    "\n",
    "train_loss_mean = 0\n",
    "test_loss_mean = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e10fcea-7037-440c-9131-6649963e42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9ab893f-7790-47ea-9056-fe8908228053",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21bb3e10-872f-4bc9-8a1b-d5c71d845a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = Encoder2d().apply({'params':enc_state, 'batch_stats':enc_batch}, x, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c41dfdb5-695b-439f-b9fd-87ff925f8f91",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlatent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f707e8-dbda-4bbb-b393-4e01a214ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = Encoder2d().apply({'params':enc_state, 'batch_stats':enc_batch}, x, rng)\n",
    "\n",
    "def loss_fn(params):\n",
    "    logits = linear_evaluation().apply(params, latent)\n",
    "    loss = jnp.mean(optax.softmax_cross_entropy(logits, y))\n",
    "    return loss, logits\n",
    "\n",
    "grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "(loss, logits), grads = grad_fn(linear_state.params)\n",
    "accuracy = jnp.mean(jnp.argmax(logits, -1) == jnp.argmax(labels, -1))\n",
    "\n",
    "return state.apply_gradients(grads=grads), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7bd8db9c-e5f3-40d3-8dae-214a715ce25b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TracerArrayConversionError",
     "evalue": "The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ShapedArray(float32[4,512])>with<DynamicJaxprTrace(level=0/1)>\nThe error occurred while tracing the function linear_train_step at /tmp/ipykernel_179226/3286191795.py:1 for jit. This concrete value was not available in Python because it depends on the values of the arguments 'enc_state', 'enc_batch', and 'x'.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2007\u001b[0m, in \u001b[0;36mshape\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2007\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m   2008\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTracerArrayConversionError\u001b[0m                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m linear_state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_train_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                  \u001b[49m\u001b[43menc_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                  \u001b[49m\u001b[43menc_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mlinear_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinear_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36mlinear_train_step\u001b[0;34m(enc_state, enc_batch, linear_state, x, y)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, logits\n\u001b[1;32m     14\u001b[0m grad_fn \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvalue_and_grad(loss_fn, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 15\u001b[0m (loss, logits), grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinear_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean(jnp\u001b[38;5;241m.\u001b[39margmax(logits, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m jnp\u001b[38;5;241m.\u001b[39margmax(labels, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mapply_gradients(grads\u001b[38;5;241m=\u001b[39mgrads), loss\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36mlinear_train_step.<locals>.loss_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(params):\n\u001b[0;32m---> 10\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean(optax\u001b[38;5;241m.\u001b[39msoftmax_cross_entropy(logits, y))\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, logits\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m~/module/model/supervised_model.py:102\u001b[0m, in \u001b[0;36mlinear_evaluation.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 102\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     x \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mleaky_relu(x)\n\u001b[1;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py:188\u001b[0m, in \u001b[0;36mDense.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;129m@compact\u001b[39m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Array) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[1;32m    178\u001b[0m   \u001b[38;5;124;03m\"\"\"Applies a linear transformation to the inputs along the last dimension.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    The transformed input.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m   kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    187\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_init,\n\u001b[0;32m--> 188\u001b[0m                       (\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures),\n\u001b[1;32m    189\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_dtype)\n\u001b[1;32m    190\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m    191\u001b[0m     bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_init, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures,),\n\u001b[1;32m    192\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_dtype)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2009\u001b[0m, in \u001b[0;36mshape\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     result \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   2008\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m-> 2009\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/core.py:534\u001b[0m, in \u001b[0;36mTracer.__array__\u001b[0;34m(self, *args, **kw)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m--> 534\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m TracerArrayConversionError(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mTracerArrayConversionError\u001b[0m: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ShapedArray(float32[4,512])>with<DynamicJaxprTrace(level=0/1)>\nThe error occurred while tracing the function linear_train_step at /tmp/ipykernel_179226/3286191795.py:1 for jit. This concrete value was not available in Python because it depends on the values of the arguments 'enc_state', 'enc_batch', and 'x'.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError"
     ]
    }
   ],
   "source": [
    "linear_state, loss = linear_train_step(\n",
    "                  enc_state=enc_state, \n",
    "                  enc_batch=enc_batch, \n",
    "                  linear_state=linear_state, \n",
    "                 x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14093d2-3abc-413d-93cb-245b76a3f708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
