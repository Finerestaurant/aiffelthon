
Loading dataset...
Traceback (most recent call last):
  File "Dilated_conv_vae_main.py", line 93, in <module>
    data = mel_dataset(dataset_dir)
  File "/home/anthonypark6904/module/dataloader.py", line 40, in __init__
    if np.load(i).shape[1] != 1876:
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py", line 413, in load
    return format.read_array(fid, allow_pickle=allow_pickle,
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/format.py", line 731, in read_array
    shape, fortran_order, dtype = _read_array_header(fp, version)
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/format.py", line 594, in _read_array_header
    header = _filter_header(header)
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/format.py", line 555, in _filter_header
    for token in tokenize.generate_tokens(StringIO(s).readline):
  File "/usr/lib/python3.8/tokenize.py", line 525, in _tokenize
    pseudomatch = _compile(PseudoToken).match(line, pos)
  File "/usr/lib/python3.8/tokenize.py", line 99, in _compile
    return re.compile(expr, re.UNICODE)
KeyboardInterrupt
Traceback (most recent call last):
  File "Dilated_conv_vae_main.py", line 93, in <module>
    data = mel_dataset(dataset_dir)
  File "/home/anthonypark6904/module/dataloader.py", line 40, in __init__
    if np.load(i).shape[1] != 1876:
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py", line 413, in load
    return format.read_array(fid, allow_pickle=allow_pickle,
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/format.py", line 731, in read_array
    shape, fortran_order, dtype = _read_array_header(fp, version)
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/format.py", line 594, in _read_array_header
    header = _filter_header(header)
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/format.py", line 555, in _filter_header
    for token in tokenize.generate_tokens(StringIO(s).readline):
  File "/usr/lib/python3.8/tokenize.py", line 525, in _tokenize
    pseudomatch = _compile(PseudoToken).match(line, pos)
  File "/usr/lib/python3.8/tokenize.py", line 99, in _compile
    return re.compile(expr, re.UNICODE)
KeyboardInterrupt